{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                age        fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "count  32561.000000  3.256100e+04   32561.000000  32561.000000  32561.000000   \n",
      "mean      38.581647  1.897784e+05      10.080679   1077.648844     87.303830   \n",
      "std       13.640433  1.055500e+05       2.572720   7385.292085    402.960219   \n",
      "min       17.000000  1.228500e+04       1.000000      0.000000      0.000000   \n",
      "25%       28.000000  1.178270e+05       9.000000      0.000000      0.000000   \n",
      "50%       37.000000  1.783560e+05      10.000000      0.000000      0.000000   \n",
      "75%       48.000000  2.370510e+05      12.000000      0.000000      0.000000   \n",
      "max       90.000000  1.484705e+06      16.000000  99999.000000   4356.000000   \n",
      "\n",
      "       hours-per-week  \n",
      "count    32561.000000  \n",
      "mean        40.437456  \n",
      "std         12.347429  \n",
      "min          1.000000  \n",
      "25%         40.000000  \n",
      "50%         40.000000  \n",
      "75%         45.000000  \n",
      "max         99.000000  \n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_train = pd.read_csv(\"adult.data.csv\")\n",
    "print(data_train.describe())\n",
    "data_train.loc[data_train['income']==' <=50K','income']=0\n",
    "data_train.loc[data_train['income']==' >50K','income']=1\n",
    "\n",
    "data_test = pd.read_csv(\"adult.test.csv\")\n",
    "data_test.loc[data_test['income']==' <=50K.','income'] = 0\n",
    "data_test.loc[data_test['income']==' >50K.','income']=1\n",
    "\n",
    "#原始数据中缺失值表示为?, pandas中缺失值为NaN,因此把所有的?替换成NaN\n",
    "data_train = data_train.replace(\" ?\",np.nan)\n",
    "data_test = data_test.replace(\" ?\",np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data scrubbing\n",
    "from scipy.stats import mode\n",
    "import random\n",
    "#填充缺失值\n",
    "#age用均值填充\n",
    "mean_age = data_train['age'].mean()\n",
    "data_train['age'].fillna(mean_age,inplace=True)\n",
    "mean_age = data_test['age'].mean()\n",
    "data_test['age'].fillna(mean_age,inplace=True)\n",
    "\n",
    "#workclass用众数填充\n",
    "workclass_most = data_train['workclass'].value_counts().index[0]\n",
    "data_train['workclass'].fillna(workclass_most,inplace=True)\n",
    "workclass_most = data_test['workclass'].value_counts().index[0]\n",
    "data_test['workclass'].fillna(workclass_most,inplace=True)\n",
    "\n",
    "#fnlwgt无缺失值不做处理\n",
    "#education用众数填充\n",
    "education_most = data_train['education'].value_counts().index[0]\n",
    "data_train['education'].fillna(education_most,inplace=True)\n",
    "education_most = data_test['education'].value_counts().index[0]\n",
    "data_test['education'].fillna(education_most,inplace=True)\n",
    "\n",
    "#education-num用众数填充\n",
    "education_num_most = data_train['education-num'].value_counts().index[0]\n",
    "data_train['education-num'].fillna(education_num_most,inplace=True)\n",
    "education_num_most = data_test['education-num'].value_counts().index[0]\n",
    "data_test['education-num'].fillna(education_num_most,inplace=True)\n",
    "\n",
    "#marital-status用众数填充\n",
    "marital_status_most = data_train['marital-status'].value_counts().index[0]\n",
    "data_train['marital-status'].fillna(marital_status_most,inplace=True)\n",
    "\n",
    "#occupation用众数填充\n",
    "occupation_most = data_train['occupation'].value_counts().index[0]\n",
    "data_train['occupation'].fillna(occupation_most,inplace=True)\n",
    "occupation_most = data_test['occupation'].value_counts().index[0]\n",
    "data_test['occupation'].fillna(occupation_most,inplace=True)\n",
    "\n",
    "#race用众数填充\n",
    "race_most = data_train['race'].value_counts().index[0]\n",
    "data_train['race'].fillna(race_most,inplace=True)\n",
    "race_most = data_test['race'].value_counts().index[0]\n",
    "data_test['race'].fillna(race_most,inplace=True)\n",
    "\n",
    "#性别随机填充，各50%的概率\n",
    "for index,value in data_train['sex'].iteritems():\n",
    "    if(pd.isnull(value)):\n",
    "        if(random.uniform(0,1)<0.5):\n",
    "            data_train.loc[index,'sex'] = \" Male\"\n",
    "        else:\n",
    "            data_train.loc[index,'sex'] = \" Female\"\n",
    "            \n",
    "for index,value in data_test['sex'].iteritems():\n",
    "    if(pd.isnull(value)):\n",
    "        if(random.uniform(0,1)<0.5):\n",
    "            data_test.loc[index,'sex'] = \" Male\"\n",
    "        else:\n",
    "            data_test.loc[index,'sex'] = \" Female\"\n",
    "\n",
    "#capital-gain使用均值填充\n",
    "mean_capital_gain = data_train['capital-gain'].mean()\n",
    "data_train['capital-gain'].fillna(mean_capital_gain,inplace=True)\n",
    "mean_capital_gain = data_test['capital-gain'].mean()\n",
    "data_test['capital-gain'].fillna(mean_capital_gain,inplace=True)\n",
    "\n",
    "#capital-loss使用均值填充\n",
    "mean_capital_loss = data_train['capital-loss'].mean()\n",
    "data_train['capital-loss'].fillna(mean_capital_loss,inplace=True)\n",
    "mean_capital_loss = data_test['capital-loss'].mean()\n",
    "data_test['capital-loss'].fillna(mean_capital_loss,inplace=True)\n",
    "\n",
    "#hours-per-week使用均值填充\n",
    "mean_hours_per_week = data_train['hours-per-week'].mean()\n",
    "data_train['hours-per-week'].fillna(mean_hours_per_week,inplace=True)\n",
    "mean_hours_per_week = data_test['hours-per-week'].mean()\n",
    "data_test['hours-per-week'].fillna(mean_hours_per_week,inplace=True)\n",
    "\n",
    "#native-country使用众数填充\n",
    "native_country_most = data_train['native-country'].value_counts().index[0]\n",
    "data_train['native-country'].fillna(native_country_most,inplace=True)\n",
    "native_country_most = data_test['native-country'].value_counts().index[0]\n",
    "data_test['native-country'].fillna(native_country_most,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#特征处理\n",
    "#age直接当做一个特征\n",
    "#workclass采用one-hot编码\n",
    "#删掉数据中的workclass这一列\n",
    "\n",
    "#将workclass one-hot编码之后加入到data_train中\n",
    "workclass = data_train['workclass']\n",
    "data_train = data_train.drop(axis=1,columns=['workclass'])\n",
    "workclass_onehot_encoder = pd.get_dummies(workclass,prefix = 'workclass')\n",
    "workclass_list = workclass_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,workclass_onehot_encoder],axis = 1)\n",
    "\n",
    "#fnlwgt直接当做特征\n",
    "\n",
    "#education做one-hot处理\n",
    "education = data_train['education']\n",
    "data_train = data_train.drop(axis=1,columns=['education'])\n",
    "education_onehot_encoder = pd.get_dummies(education,prefix = 'education')\n",
    "education_list = education_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,education_onehot_encoder],axis = 1)\n",
    "\n",
    "#education-num做one-hot处理\n",
    "education_num = data_train['education-num']\n",
    "data_train = data_train.drop(axis=1,columns=['education-num'])\n",
    "education_num_onehot_encoder = pd.get_dummies(education_num,prefix = 'education_num')\n",
    "education_num_list = education_num_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,education_num_onehot_encoder],axis=1)\n",
    "\n",
    "#marital-status做one-hot处理\n",
    "marital_status = data_train['marital-status']\n",
    "data_train = data_train.drop(axis=1,columns=['marital-status'])\n",
    "marital_status_onehot_encoder = pd.get_dummies(marital_status,prefix='marital_status')\n",
    "marital_status_list = marital_status_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,marital_status_onehot_encoder],axis=1)\n",
    "\n",
    "#occupation做one-hot处理\n",
    "occupation = data_train['occupation']\n",
    "data_train = data_train.drop(axis=1,columns=['occupation'])\n",
    "occupation_onehot_encoder = pd.get_dummies(occupation,prefix = 'occupation')\n",
    "occupation_list = occupation_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,occupation_onehot_encoder],axis = 1)\n",
    "\n",
    "#relationship one-hot\n",
    "relationship = data_train['relationship']\n",
    "data_train = data_train.drop(axis=1,columns=['relationship'])\n",
    "relationship_onehot_encoder = pd.get_dummies(relationship,prefix = 'relationship')\n",
    "relationship_list = relationship_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,relationship_onehot_encoder],axis = 1)\n",
    "\n",
    "#race one-hot\n",
    "race = data_train['race']\n",
    "data_train = data_train.drop(axis=1,columns=['race'])\n",
    "race_onehot_encoder = pd.get_dummies(race,prefix='race')\n",
    "race_list = race_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,race_onehot_encoder],axis = 1)\n",
    "\n",
    "#sex one-hot\n",
    "sex = data_train['sex']\n",
    "data_train = data_train.drop(axis=1,columns=['sex'])\n",
    "sex_onehot_encoder = pd.get_dummies(sex,prefix='sex')\n",
    "sex_list = sex_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,sex_onehot_encoder],axis = 1)\n",
    "\n",
    "#capital gain直接当成特征\n",
    "#capital loss直接当成特征\n",
    "#hours-per-week直接当成特征\n",
    "#native-country one-hot编码\n",
    "native_country = data_train['native-country']\n",
    "data_train = data_train.drop(axis=1,columns=['native-country'])\n",
    "native_country_onehot_encoder = pd.get_dummies(native_country,prefix='native_country')\n",
    "native_country_list = native_country_onehot_encoder.columns.values.tolist()\n",
    "data_train = pd.concat([data_train,native_country_onehot_encoder],axis = 1)\n",
    "columns_name_list = ['age']+workclass_list+['fnlwgt']+education_list+education_num_list+marital_status_list+ \\\n",
    "occupation_list+relationship_list+race_list+sex_list+['capital-gain']+['capital-loss']+['hours-per-week']+ \\\n",
    "native_country_list+['income']\n",
    "data_train = data_train.reindex(columns = columns_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对测试集进行特征处理\n",
    "\n",
    "data_test_encoder = []\n",
    "for index,row in data_test.iterrows():\n",
    "    tmp_data = []\n",
    "    for col_name in data_test.columns:\n",
    "        if(col_name=='age'):\n",
    "            tmp_data.append(row[col_name])\n",
    "        elif(col_name == 'fnlwgt'):\n",
    "            tmp_data.append(row[col_name])\n",
    "        elif (col_name == 'workclass'):\n",
    "            for workclass in workclass_list:\n",
    "                if(row[col_name] == workclass):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif (col_name == 'education'):\n",
    "            for education in education_list:\n",
    "                if(row[col_name] == education):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'education-num'):\n",
    "            for education_num in education_num_list:\n",
    "                if(row[col_name] == education_num):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'marital-status'):\n",
    "            for marital_status in marital_status_list:\n",
    "                if(row[col_name] == marital_status):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'occupation'):\n",
    "            for occupation in occupation_list:\n",
    "                if(row[col_name] == occupation):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'relationship'):\n",
    "            for relationship in relationship_list:\n",
    "                if(row[col_name] == relationship):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'race'):\n",
    "            for race in race_list:\n",
    "                if(row[col_name] == race):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'sex'):\n",
    "            for sex in sex_list:\n",
    "                if(row[col_name] == sex):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'capital-gain'):\n",
    "            tmp_data.append(row[col_name])\n",
    "        elif(col_name == 'capital-loss'):\n",
    "            tmp_data.append(row[col_name])\n",
    "        elif(col_name == 'hours-per-week'):\n",
    "            tmp_data.append(row[col_name])\n",
    "        elif(col_name == 'native-country'):\n",
    "            for native_country in native_country_list:\n",
    "                if(row[col_name] == native_country):\n",
    "                    tmp_data.append(1)\n",
    "                else:\n",
    "                    tmp_data.append(0)\n",
    "        elif(col_name == 'income'):\n",
    "            tmp_data.append(row[col_name])\n",
    "    data_test_encoder.append(tmp_data)\n",
    "\n",
    "data_test = pd.DataFrame(data = data_test_encoder , columns = data_train.columns.values)\n",
    "             \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Start\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "16281\n",
      "422 2850 996 12013\n",
      "predict accuracy = 0.7990295436398256\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "16281\n",
      "419 2847 999 12016\n",
      "predict accuracy = 0.7993980713715374\n",
      "time 295.77777777777777 ms\n"
     ]
    }
   ],
   "source": [
    "#获得预测结果\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "X_train = data_train.drop('income',axis = 1)\n",
    "y_train = data_train.loc[:,'income']\n",
    "X_test = data_test.drop('income',axis=1)\n",
    "y_test = data_test.loc[:,'income']\n",
    "\n",
    "print('Logistic Regression Start')\n",
    "\n",
    "c = np.array([10,5,2,1,0.5,0.1,0.05,0.005,0.001])\n",
    "\n",
    "y_test_list = np.array(y_test)\n",
    "avg_time = 0\n",
    "for t in range(0,len(c)):\n",
    "    start_time = time.time()\n",
    "    lm = LogisticRegression(penalty='l2',C=c[t])\n",
    "    lm.fit(np.array(X_train),np.array(y_train))\n",
    "    predicted  = lm.predict(np.array(X_test))\n",
    "    #scores = lm.decision_function(transformed_testing_matrix)\n",
    "    # y_pred_est = lm.predict_proba(transformed_testing_matrix)\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    array_len = len(predicted)\n",
    "    print(array_len)\n",
    "    for i in range(array_len):\n",
    "        if predicted[i] != y_test_list[i]:\n",
    "            if predicted[i] == 1 and y_test_list[i] == 0:\n",
    "                fp +=1\n",
    "            else:\n",
    "                fn += 1\n",
    "        else:\n",
    "            if predicted[i]  == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "    print(fp,fn,tp,tn)\n",
    "    print(\"predict accuracy = %s\"% str(1.0*(tp+tn)/(tp+tn+fp+fn)))\n",
    "    endtime = time.time()\n",
    "    avg_time += (endtime-start_time)\n",
    "print(\"time \"+str(int(round(avg_time*1000))/len(c)) + ' ms') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 3093 753 12333\n",
      "predict accuracy = 0.8037589828634605\n",
      "time 52.44444444444444 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "start_time = time.time()\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(np.array(X_train),np.array(y_train))\n",
    "predicted = clf.predict(np.array(X_test))\n",
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "for i in range(array_len):\n",
    "    if predicted[i] != y_test_list[i]:\n",
    "        if predicted[i] == 1 and y_test_list[i] == 0:\n",
    "            fp +=1\n",
    "        else:\n",
    "            fn += 1\n",
    "    else:\n",
    "        if predicted[i]  == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "print(fp,fn,tp,tn)\n",
    "print(\"predict accuracy = %s\"% str(1.0*(tp+tn)/(tp+tn+fp+fn)))\n",
    "endtime = time.time()\n",
    "print(\"time \"+str(int(round((endtime-start_time)*1000))/len(c)) + ' ms') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658 2672 1174 11777\n",
      "predict accuracy = 0.7954671088999448\n",
      "time 19.88888888888889 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "start_time = time.time()\n",
    "clf = GaussianNB()\n",
    "clf.fit(np.array(X_train), np.array(y_train))\n",
    "predicted = clf.predict(np.array(X_test))\n",
    "fp = 0\n",
    "fn = 0\n",
    "tp = 0\n",
    "tn = 0\n",
    "for i in range(array_len):\n",
    "    if predicted[i] != y_test_list[i]:\n",
    "        if predicted[i] == 1 and y_test_list[i] == 0:\n",
    "            fp +=1\n",
    "        else:\n",
    "            fn += 1\n",
    "    else:\n",
    "        if predicted[i]  == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            tn += 1\n",
    "print(fp,fn,tp,tn)\n",
    "print(\"predict accuracy = %s\"% str(1.0*(tp+tn)/(tp+tn+fp+fn)))\n",
    "endtime = time.time()\n",
    "print(\"time \"+str(int(round((endtime-start_time)*1000))/len(c)) + ' ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "358 3597 249 12077\n",
      "predict accuracy = 0.7570788035132977\n",
      "time 82976.66666666667 ms\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# start_time = time.time()\n",
    "# clf = SVC()\n",
    "# clf.fit(np.array(X_train),np.array(y_train))\n",
    "# predicted = clf.predict(np.array(X_test))\n",
    "# fp = 0\n",
    "# fn = 0\n",
    "# tp = 0\n",
    "# tn = 0\n",
    "# for i in range(array_len):\n",
    "#     if predicted[i] != y_test_list[i]:\n",
    "#         if predicted[i] == 1 and y_test_list[i] == 0:\n",
    "#             fp +=1\n",
    "#         else:\n",
    "#             fn += 1\n",
    "#     else:\n",
    "#         if predicted[i]  == 1:\n",
    "#             tp += 1\n",
    "#         else:\n",
    "#             tn += 1\n",
    "# print(fp,fn,tp,tn)\n",
    "# print(\"predict accuracy = %s\"% str(1.0*(tp+tn)/(tp+tn+fp+fn)))\n",
    "# endtime = time.time()\n",
    "# print(\"time \"+str(int(round((endtime-start_time)*1000))/len(c)) + ' ms') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7637737239727289\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import numpy as np  \n",
    "# from torch import nn\n",
    "# from torch import optim\n",
    "# class Neuralnetwork(nn.Module):\n",
    "#     def __init__(self, layer_dim):\n",
    "#         super(Neuralnetwork, self).__init__()\n",
    "#         self.layer = []\n",
    "#         for i in range(len(layer_dim)-1):\n",
    "#             self.layer.append(nn.Linear(layer_dim[i],layer_dim[i+1]))\n",
    "#         print(self.layer)\n",
    "\n",
    "#     def forward(self, x):   \n",
    "#         print(self.layer)\n",
    "#         for i in range(len(self.layer)):\n",
    "#             x = self.layer[i](x)\n",
    "#         return x\n",
    "\n",
    "# dim = X_train.shape[1]\n",
    "# layer_dim=[dim,300,200,100,2]\n",
    "# #hyper param\n",
    "# learning_rate = 0.01\n",
    "# model = Neuralnetwork(layer_dim)\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Neuralnetwork(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2,n_hidden_3,n_hidden_4,n_hidden_5,out_dim):\n",
    "        super(Neuralnetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, n_hidden_1)\n",
    "        self.layer2 = nn.Linear(n_hidden_1, n_hidden_2)\n",
    "        self.layer3 = nn.Linear(n_hidden_2, n_hidden_3)\n",
    "        self.layer4 = nn.Linear(n_hidden_3, n_hidden_4)\n",
    "        self.layer5 = nn.Linear(n_hidden_4, n_hidden_5)\n",
    "        self.layer6= nn.Linear(n_hidden_5, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        return x\n",
    "\n",
    "model = Neuralnetwork(X_train.shape[1], 120, 120, 120,120,120,2)\n",
    "# if torch.cuda.is_available():\n",
    "#     model = model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "torch_X_train = torch.FloatTensor(np.array(X_train))\n",
    "torch_y_train = torch.LongTensor(np.array(y_train))\n",
    "variable_X_train = Variable(torch_X_train, requires_grad=False)\n",
    "variable_y_train = Variable(torch_y_train,requires_grad=False)\n",
    "\n",
    "#\n",
    "for t in range(10):\n",
    "    \n",
    "    out = model(variable_X_train)     # 喂给 net 训练数据 x, 输出分析值\n",
    "\n",
    "    loss = loss_func(out, variable_y_train)     # 计算两者的误差\n",
    "    optimizer.zero_grad()   # 清空上一步的残余更新参数值\n",
    "    loss.backward()         # 误差反向传播, 计算参数更新值\n",
    "    optimizer.step()        # 将参数更新值施加到 net 的 parameters 上\n",
    "    predicted = torch.max(F.softmax(out,dim=1), 1)[1]\n",
    "    pred_y = predicted.data.numpy().squeeze()\n",
    "    target_y = variable_y_train.data.numpy()\n",
    "    correct = sum(pred_y == target_y)\n",
    "    error = sum(pred_y!=target_y)\n",
    "    \n",
    "#预测\n",
    "torch_X_test = torch.FloatTensor(np.array(X_test))\n",
    "torch_y_test = torch.LongTensor(np.array(y_test))\n",
    "variable_X_test = Variable(torch_X_test, requires_grad=False)\n",
    "variable_y_test = Variable(torch_y_test,requires_grad=False)\n",
    "out = model(variable_X_test)\n",
    "predicted = torch.max(F.softmax(out,dim=1),1)[1]\n",
    "pred_y = predicted.data.numpy().squeeze()\n",
    "target_y = variable_y_test.data.numpy()\n",
    "correct = sum(pred_y == target_y )\n",
    "error = sum(pred_y!=target_y)\n",
    "print(1.0*correct/(correct+error))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
